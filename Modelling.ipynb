{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933146be-fb66-4b9e-b5d9-4901e0f136f4",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eaa22b-5b11-4614-b8fd-ed5754f1ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from collections import Counter\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modelling\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import recall_score, f1_score, accuracy_score, precision_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea148b3f-2a1c-456c-a93b-20d737fbec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read-in the cleaned data from FraudDetectionEDA\n",
    "df = pd.read_csv(\"data/data_train_clean_withdummies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e90fa-9767-4cf9-9ecb-898534a00816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff608248-c28b-4282-88e8-bd536e835855",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8721e2c-52a8-43a2-8f62-bc5d909d89ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate predictor variables\n",
    "X = df.drop(['Value','FraudResult'], axis =1)\n",
    "\n",
    "# separate target variable\n",
    "y = df['FraudResult']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d92352e-3e5a-40e5-b4d5-754e157ce957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = RSEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30f8091-45ae-49bd-846b-3ad8e35c076a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Balance - Oversampling\n",
    "\n",
    "*Usefull links*:\n",
    "- [SMOTE](https://imbalanced-learn.org/stable/over_sampling.html)\n",
    "- [Handling Imbalanced Data Sets, Medium](https://medium.com/coinmonks/handling-imbalanced-datasets-predicting-credit-card-fraud-544f5e74e0fd)\n",
    "- [Dealing with imbalanced Data, towardsdatascience](https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36daf1c-c1bc-4735-aa08-a6fc9b89e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply resampling ONLY to train data\n",
    "X_train_res, y_train_res = SMOTE().fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b916c11d-771a-4517-938d-00ef980b5536",
   "metadata": {},
   "source": [
    "After we resampled the data using SMOTE, we now have to apply a scaler to `X_train_res` and `X_test`. After scalling we convert them again to a `DataFrame` for easier handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a09f9-700e-4500-afe8-78f6af487351",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "#standardization of train set (fit_transform)\n",
    "X_train_res_stand = scaler.fit_transform(X_train_res)\n",
    "\n",
    "#change array to dataframe\n",
    "scaled_df_train_resampled = pd.DataFrame(X_train_res_stand)\n",
    "scaled_df_train_resampled.columns = X_train.columns\n",
    "\n",
    "#standardization of test set (transform)\n",
    "X_test_stand = scaler.transform(X_test)\n",
    "\n",
    "#change array to dataframe\n",
    "scaled_df_test = pd.DataFrame(X_test_stand)\n",
    "scaled_df_test.columns = X_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26df8b09-a419-47cd-a9ec-d4a11486a49c",
   "metadata": {},
   "source": [
    "Now, we are set to feed our models with the *cleaned* and *resampled* data. For easier handling, we will save and read the finished data set as `X_train`, `X_test` and `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f181dc89-7d0a-4415-a484-e509f9c36717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save TRAIN sets\n",
    "scaled_df_train_resampled.to_csv('data/x-train.csv', index=False)\n",
    "y_train_res.to_csv('data/y-train.csv', index=False)\n",
    "\n",
    "# save TEST sets\n",
    "scaled_df_test.to_csv('data/x-test.csv', index=False)\n",
    "y_test.to_csv('data/y-test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec085fa-10c8-47b3-a197-91c9d75a73a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in finished data\n",
    "X_train = pd.read_csv('data/x-train.csv')\n",
    "y_train = pd.read_csv('data/y-train.csv')\n",
    "\n",
    "X_test = pd.read_csv('data/x-test.csv')\n",
    "y_test = pd.read_csv('data/y-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb02fbd-485d-46ba-acb4-cc731940694d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Base Line Model\n",
    "\n",
    "The following code for the classifier comparison was taken from [Machine Learning Mastery](https://machinelearningmastery.com/naive-classifiers-imbalanced-classification-metrics/). It shows, which type of `DummyClassifier` delivers the best results for a **F1-Score**. On the website (see link) there are more examples for **other metrics** as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a09f6-50c5-40fa-93a1-9ae08820c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare naive classifiers with f1-measure\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(model, X, y, scoring='f1', cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# define models to test\n",
    "def get_models():\n",
    "\tmodels, names = list(), list()\n",
    "\t# Uniformly Random Guess\n",
    "\tmodels.append(DummyClassifier(strategy='uniform'))\n",
    "\tnames.append('Uniform')\n",
    "\t# Prior Random Guess\n",
    "\tmodels.append(DummyClassifier(strategy='stratified'))\n",
    "\tnames.append('Stratified')\n",
    "\t# Majority Class: Predict 0\n",
    "\tmodels.append(DummyClassifier(strategy='most_frequent'))\n",
    "\tnames.append('Majority')\n",
    "\t# Minority Class: Predict 1\n",
    "\tmodels.append(DummyClassifier(strategy='constant', constant=1))\n",
    "\tnames.append('Minority')\n",
    "\t# Class Prior\n",
    "\tmodels.append(DummyClassifier(strategy='prior'))\n",
    "\tnames.append('Prior')\n",
    "\treturn models, names\n",
    "\n",
    "# define dataset\n",
    "\"\"\"\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "\tn_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)\n",
    "\"\"\"\n",
    "# define models\n",
    "models, names = get_models()\n",
    "results = list()\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "\t# evaluate the model and store results\n",
    "\tscores = evaluate_model(X_train, y_train, models[i])\n",
    "\tresults.append(scores)\n",
    "\t# summarize and store\n",
    "\tprint('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))\n",
    "# plot the results\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31a9cf-8050-49f7-bd23-8f6289503a2d",
   "metadata": {},
   "source": [
    "### Results:\n",
    "\n",
    "The 'Minority' `DummyClassifier` seems to deliver the best result (0.667) for a naive classifier with regards to the **F1-Score** metric which is our goal. The values in braces show the variance of that model. For 'Minority' it is 0.\n",
    "\n",
    "Therefore, we will procede with this `DummyClassifier` as our baseline model and compare our own models against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41c116-d51a-4896-bab9-0ca256204302",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "y_dummy_pred = dummy_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f7a1ed-bcd1-4664-8e67-dc34ea4b35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------\"*10)\n",
    "print(classification_report(y_train, y_dummy_pred))\n",
    "print(\"------\"*10)\n",
    "\n",
    "# F1-score on test set\n",
    "print(\"F1-score:\", f1_score(y_train, y_dummy_pred, average='binary'))\n",
    "print(\"------\"*10)\n",
    "\n",
    "f1_baseline = f1_score(y_train, y_dummy_pred, average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7ee686-63ff-424c-a4fc-0cb9c623fddf",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "### Basic Models\n",
    "\n",
    "1. SVM ==> Daniela\n",
    "2. LogReg ==> Kai-Yang\n",
    "3. DecisionTree ==> Fabio\n",
    "\n",
    "### Advanced Models\n",
    "\n",
    "1. Random Forest ==> Fabio\n",
    "2. AdaBoost ==> Kai-Yang\n",
    "3. Stacking ==> Kai-Yang\n",
    "\n",
    "### Results\n",
    "\n",
    "All basic models showed weak performance in comparison to our advanced models. Hence, we decided to focus on them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f2197-3ae4-4111-ba26-0da5e647b592",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25496154-f4f8-4c36-881d-2d1063337081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest =================================\n",
    "\n",
    "# Make a decision tree and train\n",
    "tree = DecisionTreeClassifier(max_depth= None, random_state=RSEED)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_tree_pred = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b62fa0-303c-41ec-8b98-18fcb7862f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Decision tree has {tree.tree_.node_count} nodes with maximum depth {tree.tree_.max_depth}.')\n",
    "print(f'Model Accuracy: {tree.score(X_train, y_train)}')\n",
    "print(f'Model F1-Score: {f1_score(y_test, y_tree_pred, average=\"binary\")}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4251b2-7c17-40fe-9122-712c81b65450",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131867d-a771-4fa9-b439-feafa3c974ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal params: n_estimators=1000, criterion=entropy, max_features=sqrt\n",
    "model = RandomForestClassifier(n_estimators=1000, \n",
    "                               random_state=RSEED, \n",
    "                               criterion = 'entropy',\n",
    "                               max_features = 'sqrt',\n",
    "                               max_depth = None,\n",
    "                               n_jobs=-1, verbose = 1)\n",
    "\n",
    "# Fit on training data\n",
    "model.fit(X_train, y_train)\n",
    "y_rf_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889aff3e-36f7-4c89-aa0f-5b628fce9c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------\"*10)\n",
    "print(classification_report(y_test, y_rf_pred))\n",
    "print(\"------\"*10)\n",
    "\n",
    "# F1-score on test set\n",
    "print(f'Model F1-Score: {f1_score(y_test, y_rf_pred, average=\"binary\")}')\n",
    "print(\"------\"*10)\n",
    "print(\"Confusion Matrix: \\n\", \n",
    "        confusion_matrix(y_test, y_rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf34243-e376-432c-814b-3c8ffc014694",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Finding optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d07bf8d-c07d-494f-9064-8f4adae90523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID SEARCH ===========================\n",
    "\n",
    "param_grid = {'max_features': ['sqrt'], \n",
    "              'criterion': ['gini', 'entropy'],\n",
    "              'n_estimators': [100, 200, 300, 400, 500],\n",
    "              'random_state': [10,42,112]\n",
    "             }\n",
    "\n",
    "grid       = GridSearchCV(RandomForestClassifier(), param_grid, verbose=True, n_jobs=-1,scoring='f1')\n",
    "model_rf  = grid.fit(X_train, np.ravel(y_train))\n",
    "y_rf_grid_pred = model_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2595fa3-cf9a-4f2d-a9df-1355c7ccec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------\"*10)\n",
    "print(classification_report(y_test, y_rf_grid_pred))\n",
    "print(\"------\"*10)\n",
    "\n",
    "# F1-score on test set \n",
    "print(f'Model F1-Score: {f1_score(y_test, y_rf_grid_pred, average=\"binary\")}')\n",
    "print(\"------\"*10)\n",
    "print(f'Model Matthew Corr: {matthews_corrcoef(y_test, y_rf_grid_pred)}')\n",
    "print(\"------\"*10)\n",
    "print(\"Confusion Matrix: \\n\", \n",
    "        confusion_matrix(y_test, y_rf_grid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b10fac-1d4d-4ea3-8976-b007fa5b9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50269299-1cbb-45d1-9f54-af810e698c71",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Feature Importance (Random Forest)\n",
    "\n",
    "For this purpose we use the in-built parameter `feature_importance_` of the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745fbff4-2c56-474b-ad80-8ae7e61add0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.asarray(model.feature_importances_)\n",
    "print(np.array_str(x, precision=2, suppress_small=True))\n",
    "print(model.feature_names_in_)\n",
    "df_feat = pd.DataFrame(x, columns= ['Importance'],index=model.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecba409-e0e7-4114-9a96-81d6890a75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a71592-7384-40f4-9a96-06d6bc43b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_plot = df_feat.query('Importance >= 0.03').reset_index().sort_values(by=['Importance'], \n",
    "                                                                             ascending=False,\n",
    "                                                                            ignore_index=True)\n",
    "x = df_feat_plot['index']\n",
    "y = df_feat_plot.Importance\n",
    "\n",
    "sns.scatterplot(x=x, y=y, marker='x', color='green')\n",
    "plt.xticks(ticks = range(4),\n",
    "           rotation=30, \n",
    "           labels=['Value', 'ProviderId 6', 'ProductId 15', 'AccountId'],\n",
    "           fontsize=10,\n",
    "           ha='right'\n",
    "          )\n",
    "#plt.setp( plt.xaxis.get_majorticklabels(), rotation=-45, ha=\"right\" )\n",
    "plt.ylabel('Relative Importance (%)', fontsize=14)\n",
    "plt.xlabel('Feature',fontsize=14)\n",
    "plt.grid(visible=True, axis='y', linestyle='--')\n",
    "plt.suptitle('Relative Feature Importance in Model', fontsize=16);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7323cfa-6a8e-4338-90e2-ed669c3d22a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c6578-35da-4b03-ae9e-2ee2c9b4ea12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
