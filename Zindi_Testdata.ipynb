{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49032cef-fe19-4a17-86f9-434387501f55",
   "metadata": {},
   "source": [
    "# Using the original Zindi test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88498a21-d887-4c84-938d-2a18d92ef99e",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee04ffb-36de-4af7-8af4-1cf19cf17105",
   "metadata": {},
   "source": [
    "In this challenge, Zindi provided training and test data. However, there are no target values (i.e. fraudulent or not fradulent) in the provided test data. Thus, in order to test the the model, a test train split of the train data was used before. This notebook describes how the actual test data provided by Zindi can be used to test the model nonetheless.\n",
    "\n",
    "_Note: In the following, all variable names containing \"test\" are written as uppercases \"TEST\" to highlight the difference between the Zindi test data and the test data from the test train split of the Zindi train data._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca367c8-7707-4736-b519-184bc6e141a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c648c816-dc0a-4748-b0f4-8b9ba9bed238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic modules and plotting tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modelling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Scikit-learn model modules\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Scikit-learn metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# Random seed used in this notebook:\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c00ad0-06b5-4613-a526-6e9b1af74e74",
   "metadata": {},
   "source": [
    "### Loading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74be5f8-76eb-446a-805d-9013f8307886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data provided by Zindi\n",
    "data_TEST = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6963f1-7d4b-4fe9-b104-36ceca43f867",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "* Stripping the ID columns from non-integer characters and converted them to integers\n",
    "* Separating TransactionStartTime into transactiontime and transactiondate\n",
    "* Dropping redundant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e94ed-ec2e-44bc-9889-690d1d54780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for stripping ID related columns\n",
    "def remove_letters(string):\n",
    "    return int(string.split('_')[1])\n",
    "\n",
    "# Applying the function above for all ID related columns\n",
    "id_columns = [\"TransactionId\",\"BatchId\",\"AccountId\",\"SubscriptionId\",\"CustomerId\",\"ProviderId\",\"ProductId\",\"ChannelId\"]    \n",
    "for i in id_columns:\n",
    "    data_TEST[i] = data_TEST[i].apply(lambda x:remove_letters(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b919f5ba-1720-4c2f-86e6-8ec049a0211f",
   "metadata": {},
   "source": [
    "### Unknotting TransactionStartTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da62bd0-21c6-43ae-9617-3a425a216c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for seperating `TransactionStartTime` into time and date, respectively\n",
    "def convert_to_date(date):\n",
    "    # convert field into datetime format\n",
    "    d = datetime.strptime(date,'%Y-%m-%dT%H:%M:%SZ')\n",
    "    # extract date\n",
    "    return d.date()\n",
    "\n",
    "def convert_to_time(date):\n",
    "    d = datetime.strptime(date,'%Y-%m-%dT%H:%M:%SZ')\n",
    "    # extract time\n",
    "    return d.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6d73b-3fe4-4450-95d5-0f149af673be",
   "metadata": {},
   "source": [
    "Consolidate times into seperate blocks:\n",
    "\n",
    "1. 00:00 - 05:59 (night)\n",
    "2. 06:00 - 09:59 (morning)\n",
    "3. 10:00 - 13:59 (midday)\n",
    "4. 14:00 - 17:59 (afternoon)\n",
    "5. 18:00 - 23:59 (evening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133837fd-fc74-4474-8b24-4b5a97eed163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day time consolidation function\n",
    "def consolidate_time(time):\n",
    "    if time.hour < 6:\n",
    "        return 'night'\n",
    "    elif time.hour < 10:\n",
    "        return 'morning'\n",
    "    elif time.hour < 14:\n",
    "        return 'midday'\n",
    "    elif time.hour < 18:\n",
    "        return 'afternoon'\n",
    "    else:\n",
    "        return 'evening'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbebb7d-5809-478c-aa18-39efa0c120a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column 'DayTime' by creatinge new columns with seperate information for `TransactionTime` and `TransactionDate`\n",
    "data_TEST['TransactionTime'] = data_TEST.TransactionStartTime.apply(lambda x: convert_to_time(x))\n",
    "data_TEST['TransactionDate'] = data_TEST.TransactionStartTime.apply(lambda x: convert_to_date(x))\n",
    "data_TEST['DayTime'] = data_TEST.TransactionTime.apply(lambda x: consolidate_time(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f7f7de-00b3-46d9-a0ee-a98257755ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column by extracting weekdays from `TransactionDate`\n",
    "data_TEST['TransactionWeekday'] = data_TEST.TransactionDate.apply(lambda x: x.isoweekday())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e45433f-09fe-4ac8-8000-9d98568a11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_isoweek(date):\n",
    "    return date.isocalendar()[1]\n",
    "\n",
    "data_TEST['ISOWeek'] = data_TEST.TransactionDate.apply(lambda x: convert_to_isoweek(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10541c-87e4-47c8-86fc-4d72d6dcee95",
   "metadata": {},
   "source": [
    "### Further feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3261250-3e13-4772-ad9f-03fb1c6090c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature to distinguish between Debit (0) and Credit (1)\n",
    "data_TEST['DebitCredit'] = data_TEST.Amount.apply(lambda x: 0 if x > 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efc3124-7d94-4c9e-9b24-cc5de197083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New column: transaction per batch\n",
    "a = data_TEST.groupby('BatchId', as_index=False)['TransactionId'].count()\n",
    "a.rename(columns= {'TransactionId': 'TransactionInBatch' }, inplace=True)\n",
    "data_TEST = data_TEST.merge(a, on='BatchId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877bd0f9-b12c-4332-b798-006fb8e50b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New column: difference between Value and Amount\n",
    "data_TEST['value_amount_diff'] = abs(data_TEST[\"Value\"] - data_TEST[\"Amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8db16-4fa5-4e81-904e-75835350ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine number of transactions to date set by same account ID:\n",
    "def transactions_toDate(df, transaction_id, account_id):\n",
    "    \"\"\"\n",
    "    returns dataframe\n",
    "    \"\"\"\n",
    "    # create empty dictionary\n",
    "    TTD = {'t_id': [], 'a_id': [], \n",
    "           'TransactionsToDate': []}#, 'date': []}\n",
    "    count = 0\n",
    "    # iterate through all transaction ids for one account id and assign counts\n",
    "    for t in transaction_id:\n",
    "        TTD['t_id'] += [t]\n",
    "        TTD['a_id'] += [account_id]\n",
    "        TTD['TransactionsToDate'] += [count]\n",
    "        count += 1\n",
    "    # return counts in data frame format \n",
    "    return pd.DataFrame.from_dict(TTD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e395c8d-6e9c-427c-b6cd-4a2b1d119bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame()\n",
    "for i in data_TEST.AccountId.unique():\n",
    "    df = data_TEST.query('AccountId == @i')\n",
    "    # count seperately for every sub set of account ids\n",
    "    TTD = transactions_toDate(df, df.TransactionId,i)\n",
    "    # add counts vertically to temporary data frame\n",
    "    temp = pd.concat([temp, TTD], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a391e4-4732-4e78-8e39-c0eaa71f49e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_TEST = data_TEST.merge(temp, left_on='TransactionId', right_on='t_id')\n",
    "data_TEST.drop(['t_id', 'a_id'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb6d5e5-b445-423f-8c99-6339517ef81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that do not convey additional meaning\n",
    "cols_to_drop = ['SubscriptionId','CurrencyCode', 'CountryCode', 'TransactionStartTime', 'BatchId','TransactionTime','Amount','TransactionDate']\n",
    "data_TEST_clean = data_TEST.drop(columns=cols_to_drop, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e3e4d8-ac37-4c8d-8f4c-3784ebbabacb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8df574-4ea5-4715-87dc-f6ed3876080e",
   "metadata": {},
   "source": [
    "Transform 'Value' due to skewness into log('Value'). The 'Value' column can therefore be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa2fbf-228d-4bde-860a-cb5585734ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_TEST_clean['ValueLog']=np.log(data_TEST_clean.Value)\n",
    "data_TEST_clean.drop(columns='Value', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60465df3-6b29-49aa-8771-49976a8225a9",
   "metadata": {},
   "source": [
    "### Preparing for model input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31377d26-f363-498d-b3ab-0c6d9aa2e97e",
   "metadata": {},
   "source": [
    "Now, the data can be used to feed the model. To simplify, data_TEST_clean is now used as df_TEST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c2a8f-3077-410d-b3a8-8a1613a29af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TEST = data_TEST_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac0149-f2ff-49ce-ba3a-b2cf31af4841",
   "metadata": {},
   "source": [
    "Dummy variables have to be created to use categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0492e56e-7260-4b87-9349-55175b68ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TEST = pd.get_dummies(df_TEST, columns = ['ProviderId', 'ProductId', 'ProductCategory', 'ChannelId', 'PricingStrategy', 'DayTime','TransactionWeekday','DebitCredit'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54bf25-e553-4726-b1cd-4595b773ddae",
   "metadata": {},
   "source": [
    "Loading the previously prepared train data as df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3cec2-b67f-4abd-8149-8c6236e95a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data_train_clean_withdummies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e812832-845d-4a0b-8110-a8ddb4b2c09e",
   "metadata": {},
   "source": [
    "Some column names existent in df are not present in df_TEST and vice versa. First, these columns are identified by compare both column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bafc01-cd09-4434-ae2a-713dca258a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = set(df.columns)\n",
    "df_TEST_columns = set(df_TEST.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd0c2b-2537-4640-b144-dd5b3bd2d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following features are missing in df_TEST\n",
    "missing_in_df_TEST = df_columns.difference(df_TEST_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a6a20-da1d-4279-925e-81f7dd5f16c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following features are missing in df\n",
    "missing_in_df = df_TEST_columns.difference(df_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec4680-cafe-4bd6-aa6f-34e25c7f9548",
   "metadata": {},
   "source": [
    "Adding missing columns except for the target variable 'FraudResult':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392547b-b309-4730-9f1f-8eee047e55e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in missing_in_df_TEST:\n",
    "    if i == \"FraudResult\":\n",
    "        continue\n",
    "    df_TEST[i] = 0\n",
    "    print(i)\n",
    "for i in missing_in_df:\n",
    "    print(i)\n",
    "    df[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc189e7-c38c-4a40-8b95-8d7761436021",
   "metadata": {},
   "source": [
    "### Model data input: Resampling and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085bd3eb-16d6-46eb-bd9b-9482f2f7051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename test data for clarification\n",
    "X_TEST = df_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8105db-3872-4665-935c-967ea3e49593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate predictor variables\n",
    "X_train = df.drop('FraudResult', axis =1)\n",
    "\n",
    "# Separate target variable\n",
    "y_train = df['FraudResult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8beba7b-01f2-41e7-a546-17cd6ae42be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply resampling ONLY to train data\n",
    "X_train_res, y_train_res = SMOTE().fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66022019-4976-47f2-91a8-bd0fe6f2585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale train and test data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#standardization of train set (fit_transform)\n",
    "X_train_res_stand = scaler.fit_transform(X_train_res)\n",
    "\n",
    "#change array to dataframe\n",
    "scaled_df_train_resampled = pd.DataFrame(X_train_res_stand)\n",
    "scaled_df_train_resampled.columns = X_train.columns\n",
    "\n",
    "#standardization of test set (transform)\n",
    "X_TEST_stand = scaler.transform(X_TEST)\n",
    "\n",
    "#change array to dataframe\n",
    "scaled_df_TEST = pd.DataFrame(X_TEST_stand)\n",
    "scaled_df_TEST.columns = X_TEST.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8170b-e32f-4691-9986-9d67195778d4",
   "metadata": {},
   "source": [
    "### Model: Stacked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0a0dd-382d-4ab9-9de9-c52ddbbfdaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub-models:\n",
    "estimators = [\n",
    "    ('dt', DecisionTreeClassifier(random_state = RSEED)),\n",
    "    ('ada', AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),n_estimators=200)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=1000,criterion = 'entropy',max_depth = None,random_state = RSEED,max_features = 'sqrt',n_jobs=-1, verbose = 1))\n",
    "    #('rf', RandomForestClassifier(n_estimators=100,random_state = RSEED,max_features = 'sqrt',n_jobs=-1, verbose = 1))\n",
    "    ]\n",
    "\n",
    "# Meta-model\n",
    "clf = StackingClassifier(estimators = estimators, final_estimator = LogisticRegression(),cv=10)\n",
    "\n",
    "# Fit the training data\n",
    "clf.fit(scaled_df_train_resampled, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb69914-06f6-4c3f-adc8-861aceec99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted y values by this model\n",
    "stack_y_pred = clf.predict(scaled_df_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb8e5f-a3b4-4171-8dd7-5ceb96ff873e",
   "metadata": {},
   "source": [
    "### Data upload to Zindi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a1dc2c-22df-45bc-a798-c62bb8e17c33",
   "metadata": {},
   "source": [
    "Accessing [this link](https://zindi.africa/competitions/xente-fraud-detection-challenge) and click on the \"Get a score\" button, the predicted data can be evaluated. The true target values are hidden such that only a final score can be obtained without knowing any details. To upload the data, the data must be prepared as shown in the template \"sample_submission.csv\" which consists of a DataFrame with two columns: TransactionId and FraudResult. This DataFrame is prepared with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df35ea5-8a99-48aa-93d8-570a595237a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the preditcted target values into a pandas DataFrame\n",
    "y_pred = pd.DataFrame(stack_y_pred)\n",
    "# Creating a DataFrame that consists of the Transaction IDs\n",
    "final_test_table = X_TEST[\"TransactionId\"]\n",
    "# Concatenating both DataFrames, resulting in the required two-column-DataFrame\n",
    "final_test_table = pd.concat([final_test_table,y_pred],axis=1)\n",
    "# Rename column\n",
    "final_test_table.rename(columns={0:\"FraudResult\"}, inplace=True)\n",
    "# Adding the string \"TransactionId_\" in each observation to fit the requirements\n",
    "final_test_table[\"TransactionId\"] = final_test_table[\"TransactionId\"].apply(lambda x: \"TransactionId_\"+str(x))\n",
    "# Save the data as csv\n",
    "final_test_table.to_csv('data/model_output.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
